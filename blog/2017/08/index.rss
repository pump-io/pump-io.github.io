<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[pump.io blog - August - 2017]]></title><description><![CDATA[pump.io blog - August - 2017]]></description><link>http://pump.io/blog/2017/08</link><generator>stratic-indexes-to-rss</generator><lastBuildDate>Wed, 06 Sep 2017 05:51:24 GMT</lastBuildDate><atom:link href="http://pump.io/blog/2017/08/index.rss" rel="self" type="application/rss+xml"/><copyright><![CDATA[Â© Copyright 2016-2017 pump.io contributors.]]></copyright><webMaster><![CDATA[AJ Jordan <alex@strugee.net>]]></webMaster><item><title><![CDATA[Zero-downtime restarts have landed]]></title><description><![CDATA[<p>I'm thrilled to announce that zero-downtime restarts, which I've been hacking on for the past week or two, <a href="https://github.com/pump-io/pump.io/pull/1406">have just landed</a> in pump.io master!</p>
<p>Zero-downtime restarts require at least two cluster workers and MongoDB as a Databank driver (we'll eventually relax the latter requirement as we continue to test the feature). Here's how it works:</p>
<ol>
<li>An administrator sends SIGUSR2 to the master pump.io process (note that SIGUSR1 is <a href="https://nodejs.org/api/process.html#process_signal_events">reserved by Node.js</a>)</li>
<li>The master process builds a queue of worker processes that need to be restarted</li>
<li>The master process picks a random worker from the queue and sends it a signal asking it to gracefully shut down</li>
<li>The worker process shuts down its HTTP server, which causes it to stop accepting new connections - it will do the same for the bounce server, if applicable</li>
<li>The worker shuts down its database connection once the HTTP server is completely shut down, meaning that it's done servicing in-flight requests</li>
<li>The worker closes its connection with the master process and Node.js automatically terminates due to there being no listeners on the event loop</li>
<li>The master recognizes the death of the worker process, replaces it, waits for the new worker to signal that it's listening for connections, and repeats from step 3 until the queue is empty</li>
</ol>
<p>This works because only one worker is shut down at a time, allowing the other workers to continue servicing requests while the one worker is restarted. We wait until the new worker actually signals it's ready to process requests before beginning the process for another worker.</p>
<p>Such a feature requires careful error handling, so there are a lot of built-in checks to prevent administrators from shooting themselves in the foot:</p>
<ul>
<li>If there's a restart already in progress, SIGUSR2 is ignored</li>
<li>If there's only 1 cluster worker, the restart request is refused (because there would be downtime and you should just restart the master)</li>
<li>
<p>The master process will load a magic number from the <em>new</em> code and compare it with the <em>old</em> magic number loaded when the master process started - if they don't match, SIGUSR2 will be refused. This number will be incremented for things that would make zero-downtime restarts cause problems, for example:</p>
<ul>
<li>The logic in the master process itself changing</li>
<li>Cross-process logic changing, such that a new worker communicating with old workers would cause problems</li>
<li>Database changes</li>
</ul>
</li>
<li>If a worker process doesn't shut itself down within 30 seconds, it will be killed</li>
<li>
<p>If a zero-downtime restart fails for any reason, the master process will refuse SIGUSR2 and will not respawn any more cluster workers, even if they crash - this is because something must have gone <em>seriously</em> wrong, either with the master, the workers, or the new code, and it's better to just restart everything. Currently this condition occurs when:</p>
<ul>
<li>A new worker died directly after being spawned (e.g. from invalid JSON in <code>pump.io.json</code>)</li>
<li>A new worker signaled that it couldn't bind to the appropriate ports</li>
</ul>
</li>
</ul>
<p>While these checks do a lot to catch problems, they're not a silver bullet, and we strongly recommend that administrators watch their logs as they trigger restarts. However, this is still a <em>huge</em> win for the admin experience - the most exciting part of this for me is that it's the first step we need to take towards having fully automatic updates, which has been a dream of mine for a long while now.</p>
<p>Admins running from git master can start experimenting with this feature today, and it will be released during the <em>next</em> release cycle - i.e. with the 5.1 beta and stable, <em>not</em> the current 5.0 beta. Since this is highly experimental, we want this to have as much time for testing as possible. You can also check out the <a href="https://pumpio.readthedocs.io/en/latest/administration/zero-downtime-restarts.html">official documentation</a> on this feature.</p>
<p>I hope people enjoy this! And as always, feel free to <a href="https://github.com/pump-io/pump.io/issues/new">report any bugs</a>.</p>
]]></description><link>http://pump.io/blog/2017/08/zero-downtime-restarts-have-landed</link><guid isPermaLink="true">http://pump.io/blog/2017/08/zero-downtime-restarts-have-landed</guid><category><![CDATA[code]]></category><pubDate>Fri, 18 Aug 2017 09:01:16 GMT</pubDate></item><item><title><![CDATA[pump.io 5.0 beta released]]></title><description><![CDATA[<p>I'm excited to announce that pump.io 5.0.0 is now officially in beta!</p>
<p>This is another big release and makes a wide variety of improvements. Here are some highlights from <a href="https://github.com/pump-io/pump.io/blob/master/CHANGELOG.md#500-beta-0---2017-08-07">the changelog</a>:</p>
<ul>
<li>More complete documentation</li>
<li>Small improvements to the administrator experience</li>
<li>A better web UI, including some user experience polishing as well as an upgrade to more performant and better-licensed libraries</li>
<li>A fix for crashes related to "login with remote account" (although this one was backported in 4.1.1)</li>
<li>Significant security improvements in the <a href="https://pumpio.readthedocs.io/en/latest/administration/upstream-systemd-unit.html">systemd service</a> shipped with the package</li>
<li>Lots of internal refactoring and simplification made possible by dropping Node 0.10/0.12 support</li>
</ul>
<p>Many of these changes - particularly the systemd changes and the fact that (<a href="http://pump.io/blog/2017/07/pump.io-4.1-is-out">as previously announced</a>) Node 0.10 and 0.12 are no longer supported - will require administrator intervention. Be sure to read our <a href="https://pumpio.readthedocs.io/en/latest/upgrades/4.x-to-5.x.html">upgrade guide</a> for details on how to deal with these changes.</p>
<p>All of these features add up to make pump.io 5.0 beta the most stable and secure release yet. As always, it will go through our beta period for about a month before being released as a fully stable version. If you try it out, the <a href="https://github.com/pump-io/pump.io/wiki/Community">community</a> would love to hear about it - and be sure to <a href="https://github.com/pump-io/pump.io/issues">report any bugs</a> you encounter!</p>
]]></description><link>http://pump.io/blog/2017/08/pump.io-5.0-beta-released</link><guid isPermaLink="true">http://pump.io/blog/2017/08/pump.io-5.0-beta-released</guid><category><![CDATA[releases]]></category><pubDate>Mon, 07 Aug 2017 18:51:12 GMT</pubDate></item></channel></rss>